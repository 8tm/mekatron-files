{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Właściciel tego notatnika: Tadejro**"
      ],
      "metadata": {
        "id": "jA6cm9R09RN4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdCU_lNi_n5y"
      },
      "source": [
        "## Zanim zaczniesz to [kliknij tu i przeczytaj wszystko](https://colab.research.google.com/drive/1wcAU5nNtFXas6euBA9Hf-jCtygZDWET9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWb0EDQDEw_I",
        "outputId": "e6f0dc45-c6a9-4377-947b-fc7605f6039b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n",
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "#@title # Informacje o karcie graficznej i wersję pythona\n",
        "_, card = !nvidia-smi --query-gpu=gpu_name --format='csv'\n",
        "print(card)\n",
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mM0CMLV71vyF"
      },
      "outputs": [],
      "source": [
        "#@title NIE URUCHAMIAĆ (do wywalenia) Łączenie się z dyskiem starym sposobem (emulacja)\n",
        "import os\n",
        "from os.path import exists\n",
        "os.chdir('/content')\n",
        "if not exists('drive'):\n",
        "    #@title Łączenie się z dyskiem\n",
        "    !sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "    !sudo apt update >/dev/null 2>&1 && sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "    !google-drive-ocamlfuse\n",
        "    !sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser\n",
        "    !xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser\n",
        "    %cd /content\n",
        "    !mkdir drive\n",
        "    %cd drive\n",
        "    !mkdir \"MyDrive\"\n",
        "    !google-drive-ocamlfuse \"/content/drive/MyDrive\"\n",
        "    %cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83yHBhE4eVRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbddef7c-2a4b-42a9-d7b3-4233643d969f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "#@title Łączenie się z dyskiem nowym sposobem (możliwe tylko na swoje konto Google)\n",
        "from google.colab import drive\n",
        "import os\n",
        "from os.path import exists\n",
        "#if not exists('drive'):\n",
        "drive.mount('drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daq1AQgCOGr2",
        "outputId": "e051f24c-283d-4d34-bcdc-70d55e023616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ścieżka do folderu głównego: /content/drive/MyDrive/mekatron/Geralt\n",
            "Ścieżka do pliku z listą plików walidacyjnych: /content/drive/MyDrive/mekatron/Geralt/list_val.txt\n",
            "Ścieżka do pliku z listą plików treningowych: /content/drive/MyDrive/mekatron/Geralt/list_train.txt\n",
            "Ścieżka do archiwum z plikami dźwiękowymi: /content/drive/MyDrive/mekatron/Geralt/wavs.7z\n",
            "Ścieżka do pliku wyjściowego: /content/drive/MyDrive/mekatron/Geralt/Geralt.tt2\n"
          ]
        }
      ],
      "source": [
        "#@markdown #Wstępna inicjalizacja\n",
        "#@markdown **voice_name** - Nazwa folderu w folderze **mekatron** na dysku, gdzie są potrzebne dane + nazwa wyjściowego modelu mekatrona\n",
        "from os.path import exists\n",
        "voice_name = \"Geralt\" #@param{type:\"string\"}\n",
        "main_folder = '/content/drive/MyDrive/mekatron'\n",
        "\n",
        "if not exists(f'{main_folder}/{voice_name}'):\n",
        "    raise NotADirectoryError('Nie ma takiego folderu. Utwórz go i przenieś co potrzebne.')\n",
        "#@markdown **train_list_name** - Nazwa listy treningowych plików dźwiękowych\n",
        "train_list_name = \"list_train.txt\" #@param{type:'string'}\n",
        "#@markdown **validation_list_name** - Nazwa listy walidacyjnych plików dźwiękowych\n",
        "\n",
        "validation_list_name = \"list_val.txt\" #@param{type:'string'}\n",
        "#@markdown **archive_name** - Nazwa archiwum z plikami dźwiękowymi.\n",
        "archive_name = \"wavs.7z\" #@param{type:'string'}\n",
        "\n",
        "\n",
        "names_list = [voice_name, validation_list_name, train_list_name, archive_name]\n",
        "\n",
        "for i in names_list:\n",
        "    if i == \"\":\n",
        "        raise ValueError(\"Wszystkie ciągi znaków muszą być wypełnione\")\n",
        "    if not exists(f\"{main_folder}/{voice_name}/{i}\") and i != voice_name:\n",
        "        raise FileNotFoundError(f\"{main_folder}/{voice_name}/{i}: Nie znaleziono podanego pliku.\")\n",
        "print(f\"Ścieżka do folderu głównego: {main_folder}/{voice_name}\")\n",
        "print(f\"Ścieżka do pliku z listą plików walidacyjnych: {main_folder}/{voice_name}/{validation_list_name}\")\n",
        "print(f\"Ścieżka do pliku z listą plików treningowych: {main_folder}/{voice_name}/{train_list_name}\")\n",
        "print(f\"Ścieżka do archiwum z plikami dźwiękowymi: {main_folder}/{voice_name}/{archive_name}\")\n",
        "print(f\"Ścieżka do pliku wyjściowego: {main_folder}/{voice_name}/{voice_name}.tt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_TrNEnV6gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b56a55f-57d1-4922-c34f-0085648d9e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gdown==4.6.6\n",
            "  Downloading gdown-4.6.6-py3-none-any.whl (14 kB)\n",
            "Collecting unidecode==1.3.6\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.9/235.9 KB 13.9 MB/s eta 0:00:00\n",
            "Collecting tensorboardX==2.6\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 KB 17.2 MB/s eta 0:00:00\n",
            "Collecting pyunpack==0.3\n",
            "  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n",
            "Collecting patool==1.12\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.5/77.5 KB 3.1 MB/s eta 0:00:00\n",
            "Collecting pynvml==11.5.0\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 KB 7.9 MB/s eta 0:00:00\n",
            "Collecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.9/183.9 KB 25.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown==4.6.6) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown==4.6.6) (3.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown==4.6.6) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown==4.6.6) (4.65.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown==4.6.6) (2.27.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX==2.6) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX==2.6) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX==2.6) (23.0)\n",
            "Collecting easyprocess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting entrypoint2\n",
            "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (1.1.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (4.4.2)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 93.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa==0.8.0) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.43.0->librosa==0.8.0) (67.6.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.43.0->librosa==0.8.0) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch>=1.0->librosa==0.8.0) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.9.0->librosa==0.8.0) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown==4.6.6) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown==4.6.6) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown==4.6.6) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown==4.6.6) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown==4.6.6) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown==4.6.6) (1.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0) (2.21)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py): started\n",
            "  Building wheel for librosa (setup.py): finished with status 'done'\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201393 sha256=0c1cff7c166e598d790da94001fce083a67205fd8b772ff48528d2c305e7bfa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/09/cc/728ed681f0fa5c37e0fbfc66d2ba07058dd995784f1f6554a8\n",
            "Successfully built librosa\n",
            "Installing collected packages: patool, entrypoint2, easyprocess, unidecode, tensorboardX, pyunpack, pynvml, resampy, librosa, gdown\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0.post2\n",
            "    Uninstalling librosa-0.10.0.post2:\n",
            "      Successfully uninstalled librosa-0.10.0.post2\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.4\n",
            "    Uninstalling gdown-4.6.4:\n",
            "      Successfully uninstalled gdown-4.6.4\n",
            "Successfully installed easyprocess-1.1 entrypoint2-1.1 gdown-4.6.6 librosa-0.8.0 patool-1.12 pynvml-11.5.0 pyunpack-0.3 resampy-0.4.2 tensorboardX-2.6 unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "#@title #Pobieranie zależności\n",
        "%%sh\n",
        "pip install gdown==4.6.6 unidecode==1.3.6 tensorboardX==2.6 pyunpack==0.3 patool==1.12 pynvml==11.5.0 librosa==0.8.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udWdAivUerNO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Instalacja Tacotrona i Waveglow\n",
        "# %tensorflow_version 2.x\n",
        "import argparse\n",
        "import gdown\n",
        "import locale\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from distutils.dir_util import copy_tree\n",
        "from gdown import download\n",
        "from math import e\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pyunpack import Archive\n",
        "from numpy import finfo\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "\n",
        "git_repo_url = 'https://github.com/8tm/mekatron2.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "\n",
        "if not exists(project_name):\n",
        "    # clone and install\n",
        "    !git clone -q --recursive {git_repo_url}\n",
        "\n",
        "\n",
        "sys.path.append(join(project_name, 'waveglow/'))\n",
        "sys.path.append(project_name)\n",
        "d = 'https://drive.google.com/uc?id='\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wIeuIzt-1Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Inicjacja submodułów, kopiowanie list\n",
        "# %tensorflow_version 2.x\n",
        "\n",
        "os.chdir('mekatron2')\n",
        "!mkdir \"filelists\"\n",
        "!cp {main_folder}/{voice_name}/{train_list_name} 'filelists'\n",
        "!cp {main_folder}/{voice_name}/{validation_list_name} 'filelists'\n",
        "!git submodule init\n",
        "!git submodule update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrWUS4jCeyDh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Tworzenie folderu wavs\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWVlvK3qvAVx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Rozpakowanie plików\n",
        "archive_path = f'/content/drive/MyDrive/mekatron/{voice_name}/{archive_name}'\n",
        "Archive(archive_path).extractall('/content/mekatron2/wavs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7-Qq1UFwv94",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbb4a45-e956-4150-cccb-24dd47afe35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mekatron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/content/mekatron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
          ]
        }
      ],
      "source": [
        "#@title # Ścieżka do modelu #\n",
        "%matplotlib inline\n",
        "\n",
        "import layers\n",
        "from distributed import apply_gradient_allreduce\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "\n",
        "\n",
        "if os.getcwd() != '/content/mekatron2':\n",
        "    os.chdir('/content/mekatron2')\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=r\".*The given NumPy array is not writeable.*\", category=UserWarning)\n",
        "\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "    tacotron_link = 'https://drive.google.com/file/d/{}/view?usp=sharing'\n",
        "    gdown.download(tacotron_link.format(file_id),file_name, quiet=True, fuzzy=True)\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            return\n",
        "            #raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename,\n",
        "            #    sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "        iteration, filepath))\n",
        "    try:\n",
        "        torch.save({'iteration': iteration,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "    print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    #@markdown **epochs_to_save** określa, co ile epok będzie zapisywany postęp treningu.\n",
        "    epochs_to_save =  150#@param{type:'number'}\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"pretrained_model\")\n",
        "      #@markdown Tutaj podajesz ID  modelu od którego zaczynasz trening\n",
        "      pretrained_model_id = \"1sv0oAfCIpXdI-kUB_1qDrPZlJ5KD65Pt\" #@param{type:'string'}\n",
        "      download_from_google_drive(pretrained_model_id ,\"pretrained_model\")\n",
        "      model = warm_start_model(\"pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "\n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    #@markdown **Tryb uproszczony** - Zmniejsza ilość informacji na wyjściu do koniecznego minimum\n",
        "    simplified_view = True #@param{type:'boolean'}\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    if not simplified_view:\n",
        "        for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "            print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "            start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "            for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "                start = time.perf_counter()\n",
        "                if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "                else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "                learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = learning_rate\n",
        "\n",
        "                model.zero_grad()\n",
        "                x, y = model.parse_batch(batch)\n",
        "                y_pred = model(x)\n",
        "\n",
        "                loss = criterion(y_pred, y)\n",
        "                if hparams.distributed_run:\n",
        "                    reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "                else:\n",
        "                    reduced_loss = loss.item()\n",
        "                if hparams.fp16_run:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                if hparams.fp16_run:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                        amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                    is_overflow = math.isnan(grad_norm)\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                        model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                if not is_overflow and rank == 0:\n",
        "                    duration = time.perf_counter() - start\n",
        "                    logger.log_training(\n",
        "                        reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                    #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "                iteration += 1\n",
        "            validate(model, criterion, valset, iteration,\n",
        "                    hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                    hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "            if epoch % epochs_to_save == 0:\n",
        "                save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "            if log_directory2 != None:\n",
        "                copy_tree(log_directory, log_directory2)\n",
        "    else:\n",
        "        for epoch in range(epoch_offset, hparams.epochs):\n",
        "            print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                start = time.perf_counter()\n",
        "                if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "                else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "                learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = learning_rate\n",
        "\n",
        "                model.zero_grad()\n",
        "                x, y = model.parse_batch(batch)\n",
        "                y_pred = model(x)\n",
        "\n",
        "                loss = criterion(y_pred, y)\n",
        "                if hparams.distributed_run:\n",
        "                    reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "                else:\n",
        "                    reduced_loss = loss.item()\n",
        "                if hparams.fp16_run:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                if hparams.fp16_run:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                        amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                    is_overflow = math.isnan(grad_norm)\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                        model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                if not is_overflow and rank == 0:\n",
        "                    duration = time.perf_counter() - start\n",
        "                    logger.log_training(\n",
        "                        reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                    #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "                iteration += 1\n",
        "            validate(model, criterion, valset, iteration,\n",
        "                    hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                    hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "            if epoch % epochs_to_save == 0:\n",
        "                save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "            if log_directory2 != None:\n",
        "                copy_tree(log_directory, log_directory2)\n",
        "\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = f'/content/drive/MyDrive/mekatron/{voice_name}/{voice_name}'  # Location to save Checkpoints\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs/Astrid' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Civepnpkw-o_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Warm Start #\n",
        "#@markdown Jeżeli zaznaczone, to trening zacznie się od nowa na bazie pretrenowanego modelu.\n",
        "\n",
        "#@markdown **Zaznaczyć**, jeżeli trening jest zaczynany\n",
        "\n",
        "#@markdown **Odznaczyć, jeżeli trening jest kontynuowany.**\n",
        "warm_start=True #@param{type:\"boolean\"}\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTd1hj1xFEZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Konfiguracja list\n",
        "hparams.training_files = f\"/content/mekatron2/filelists/list_train.txt\"\n",
        "hparams.validation_files = f\"/content/mekatron2/filelists/list_val.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDNldDnUxQMz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Prosta konfiguracja treningu #\n",
        "# hparams to Tune\n",
        "\n",
        "# These two are the most important\n",
        "#@markdown **batch_size** odpowiada za to, ile weźmie algorytm próbek naraz, co wpływa na szybkość treningu.\n",
        "\n",
        "#@markdown Jeżeli będzie zbyt duży, to skrypt się wysypie z powodu braku pamięci i trzeba będzie kliknąć Runtime -> Restart Runtime oraz zmniejszyć ten parametr.\n",
        "\n",
        "#@markdown Wyznaczany eksperymentalnie. K80 ma mniej pamięci niż T4, więc mniejszy batch_size trzeba ustawić.\n",
        "\n",
        "#@markdown Jeżeli batch_size przekracza ilość wpisów listy treningowej, to batch_size będzie równy ilości wpisów do zbioru treningowego\n",
        "hparams.batch_size =  6#@param{type:'number', min:0}\n",
        "max_length = len(list(iter(open(\"./filelists/list_train.txt\").readlines())))\n",
        "\n",
        "if  hparams.batch_size > max_length:\n",
        "    hparams.batch_size = max_length\n",
        "#@markdown **epochs** oznacza, ile razy AI będzie zaglądać do plików zanim automatycznie skończy trening.\n",
        "\n",
        "\n",
        "#@markdown Najlepiej na bieżąco monitorować w syntezatorze rezultaty i robić kopie zapasowe tych, które brzmią sensownie jak na długość datasetu.\n",
        "hparams.epochs =  2000#@param{type:'number', min:0, max:99999}\n",
        "\n",
        "#The rest aren't that important\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "hparams.A_ = 5e-4                   # Start/Max Learning Rate 5e-4 base\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
        "generate_mels = True # Don't change\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir/Delvin' # Location to save Checkpoints\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs/Delvin' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Unl2-KkwxWH9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Ustawienie ściezki\n",
        "checkpoint_path = f'/content/drive/MyDrive/mekatron/{voice_name}/{voice_name}.tt2'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8e497db92f234fb5bb79a82c3a18adad",
            "023588ac43024c2db788032eb358f625",
            "5981a2af5ca0460baa46f64041a669c5",
            "ddf0e043526c44d3bf92121063a70ad2",
            "185ca6c126974bec9ccb253adc421614",
            "f30bbc1cc15d493cad15c5e275a51b05",
            "e0eeb534cf6845cca12fdd5a0bc67dc9",
            "7ccbdeca48734e73b36c5a31be469489",
            "7a07c896db1f45e99aa46f0eaf2929f9",
            "da1254dc108248a98a7c268a650292c0",
            "14114720caef4e2e96ceec8883c2ce63"
          ]
        },
        "id": "tkl8OG3hxuXm",
        "outputId": "a979918c-21f6-4597-c66e-b841b1e9b293",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Mels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/19366 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e497db92f234fb5bb79a82c3a18adad"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title # Generowanie Meli\n",
        "if generate_mels:\n",
        "    create_mels()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAH4_itbx7uq",
        "outputId": "7fc9f9d5-c549-4772-e974-1010dc489363",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Training Files\n",
            "wavs/0x00119aae.npy|O? \n",
            "[info] has no/very little text.\n",
            "wavs/0x0011a4ee.npy|To nie… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f473.npy|„Syanna dała mi dziś laurkę...” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f48b.npy|„Syannę badali dziś czarodzieje przysłani przez Kapitułę.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f48d.npy|„Nie wiem, do jakich wniosków doszli, ale Jego Książęca Mość jest wyraźnie zmartwiony.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f491.npy|Albo przypięto jej łatkę. Czytam dalej: „Spytałam księcia, co teraz czeka księżniczkę.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f493.npy|„Nie odpowiedział.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f495.npy|„Uczyłam dziś dziewczynki języka nilfgaardzkiego...” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f49d.npy|„Syanna bardzo się przykładała, mimo że z trudem zapamiętuje nowe słówka. Księżniczka Anarietta wykazuje większy talent, ale i skłonność do psot.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011f49f.npy|„Kiedy myślała, że jej nie słyszę, powiedziała pod moim adresem: 'bloede hoel'. Śmiały się z Syanną w niebogłosy.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00119d4f.npy|Ech… Zawsze coś się musi… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00121967.npy|Sądzisz, że to dobry pomysł? W końcu to my zniweczyliśmy jej misterny plan. Myślisz, że będzie chciała… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011bb5c.npy|A może... Kto wie? Będę wolał wiedźmiński szlak i patrzenie nocą w gwiazdy nad traktem w towarzystwie potworów… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011d5df.npy|Beauclair jest wolne od Bestii… Dettlaff zginął, Syanna też nie żyje… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00122262.npy|Wiem, że to drażliwy i bolesny temat… Ale jestem ci wdzięczny. To, że pomogłeś mi z Dettlaffem… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00122268.npy|Wybacz wścibstwo, ale dlaczego… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011cc58.npy|„Adélaïde de Casteldaccia. Żona, matka, babcia, prababcia, praprababcia. Odeszła zbyt wcześnie.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011cc5c.npy|„Bob. Książęcy ludwisarz, twórca słynnych dzwonów z Beauclair.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011cc5d.npy|„Śmierć to samotność i wolność ostatnia.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011d3ec.npy|Spokojnie, dlatego tu je… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011d201.npy|Mam dobre wieści, pa… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010cca1.npy|Mógłbyś wtedy nazwać swój obraz „Wiedźmin leżący bez zmysłów wśród łanów burzanu.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011dc56.npy|„Drogi przyjacielu…\" \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011dc4f.npy|„Kochany Geralcie...\" \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011dc52.npy|„Najdroższy!\" \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011d1b7.npy|*ekhm* \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010a94f.npy|Hmm. Jeśli obiecasz, że nie będziesz mnie ciągnąć za włosy… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00105ef4.npy|Ruszajcie pod młyn, świnki. I zaczekajcie tam na mnie \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010fe4d.npy|No, no, weselisko jak się patrzy. Stoły zastawione, a i widzę, że i napić się będzie z kim... no i panny jak marzenie… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010fdee.npy|Sporo. I to nie byle jakie czupiradła, ale naprawdę kobiety z klasą. Oczywiście żadna z nich nie dorastała ci do pięt… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00110852.npy|Wszystko przez to niewydarzone ciało. Żylaste jakieś, no i ta gęba... Gdybym był całkiem sobą – o, to co innego. Ech, szkoda, że mnie wtedy nie widziałaś… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0011460a.npy|Jestem jasnowidzem… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010b8bd.npy|Racja, starczy już tych uprzejmości. Widzisz, muszę zdobyć Dom Maksymiliana Borsody’ego, cokolwiek to… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0010cbcf.npy|Tak jak mówiłem… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00112b3c.npy|Bo to obraz van Rogha. A wy podobno… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00112d1d.npy|Czara Ognia. Ciekawe, co to takiego… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00110eed.npy|Olgierd” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x000671f2.npy|Nie znam żadnego hasła. Jeśli je przypadkiem podałem, to marni z was spiskowcy. „Mdli mnie od kadzideł”, też coś… \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0008d00e.npy|Ciri, czekaj. W tym gnieździe coś- \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00106a31.npy|Jak to mówią bandyci z Velen – „zrobię z ciebie szczątki.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0007721b.npy|*khem* \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x001016a4.npy|Jakaś zmięta kartka. „... lecz wspomnisz, okrutna, serce, któreś złamała odmowy słowem ostrem...” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00106087.npy|„Pieśń o Branie Zdobywcy.” \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0008cf25.npy|*gwizd* \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x00064fec.npy|Tu masz plamkę \n",
            "[info] has no ending punctuation.\n",
            "wavs/0x0008557e.npy|...„i tak umieram w ciemności, z głodu i pragnienia... Bogowie, bądźcie łaskawi...” \n",
            "[info] has no ending punctuation.\n",
            "Checking Validation Files\n",
            "Finished Checking\n"
          ]
        }
      ],
      "source": [
        "#@title # Weryfikacja datasetu\n",
        "check_dataset(hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS_lIlic3EV6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "ea01ac58-c6da-4574-dad9-8690bf37568a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 Run: False\n",
            "Dynamic Loss Scaling: True\n",
            "Distributed Run: False\n",
            "cuDNN Enabled: True\n",
            "cuDNN Benchmark: False\n",
            "Warm starting model from checkpoint 'pretrained_model'\n",
            "\n",
            "Starting Epoch: 0 Iteration: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1c84955da5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_directory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-16ee90de4a02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title # Trening\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "\n",
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP0iFRBgUmSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e5f243-784c-4cd2-b5a1-0d2393bf0bee",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total    : 16106127360\n",
            "free     : 12767330304\n",
            "used     : 3338797056\n",
            "used per batch_size: 0.51824951171875\n"
          ]
        }
      ],
      "source": [
        "#@title # Podsumowanie\n",
        "from pynvml import *\n",
        "nvmlInit()\n",
        "h = nvmlDeviceGetHandleByIndex(0)\n",
        "info = nvmlDeviceGetMemoryInfo(h)\n",
        "print(f'total    : {info.total}')\n",
        "print(f'free     : {info.free}')\n",
        "print(f'used     : {info.used}')\n",
        "print(f'used per batch_size: {info.used / hparams.batch_size / 1024**3}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e497db92f234fb5bb79a82c3a18adad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_023588ac43024c2db788032eb358f625",
              "IPY_MODEL_5981a2af5ca0460baa46f64041a669c5",
              "IPY_MODEL_ddf0e043526c44d3bf92121063a70ad2"
            ],
            "layout": "IPY_MODEL_185ca6c126974bec9ccb253adc421614"
          }
        },
        "023588ac43024c2db788032eb358f625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f30bbc1cc15d493cad15c5e275a51b05",
            "placeholder": "​",
            "style": "IPY_MODEL_e0eeb534cf6845cca12fdd5a0bc67dc9",
            "value": "100%"
          }
        },
        "5981a2af5ca0460baa46f64041a669c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ccbdeca48734e73b36c5a31be469489",
            "max": 19366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a07c896db1f45e99aa46f0eaf2929f9",
            "value": 19366
          }
        },
        "ddf0e043526c44d3bf92121063a70ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1254dc108248a98a7c268a650292c0",
            "placeholder": "​",
            "style": "IPY_MODEL_14114720caef4e2e96ceec8883c2ce63",
            "value": " 19366/19366 [04:37&lt;00:00, 72.96it/s]"
          }
        },
        "185ca6c126974bec9ccb253adc421614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30bbc1cc15d493cad15c5e275a51b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0eeb534cf6845cca12fdd5a0bc67dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ccbdeca48734e73b36c5a31be469489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a07c896db1f45e99aa46f0eaf2929f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da1254dc108248a98a7c268a650292c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14114720caef4e2e96ceec8883c2ce63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}